# ai-inference-systems
Practical implementations and benchmarks for large-scale AI inference systems â€” focusing on latency, throughput, and deployment trade-offs.

## 01_minimal-llm-inference-engine
This project demonstrates an end-to-end inference pipeline focusing on latency, throughput, and system-level tradeoffs across CPU, GPU, ONNX, and TensorRT.
*No training code is included by design.*
